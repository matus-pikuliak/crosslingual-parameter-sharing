# Preprocessing

Slova pri spracovani na urovni znakov berieme do maximalnej dlzky 30.
Dlzka vety je maximalne 70, minimalne 2.

## POS
- vsetko z UD datasetu
- pri CZ sme spojili vsetky train do jedneho
- niektore mali rozlicne spojene slova (de el > del v spanielcine), tu sme zobrali len rozobrane slova.
- v anglickom boli niektore slova presunute, pokial sa na to odkazovalo v text neskor (8.1), tieto som ignoroval

## NER
- english - CoNLL'03 (PER, LOC, ORG, MISC)
    - toto neplati, zobral som nejaky iny dataset a dal som ho 80-10-10 - treba pozriet preprocessing script
- spanish - CoNLL'02 (PER, LOC, ORG, MISC)
- german - germeval (PER, LOC, ORG, MISC)
- czech - czech named entity corpus (p - PER, g - LOC, i+ms - ORG, oa+op - MISC)
- only first level annotation (v Germeval mali aj second level, cize napr. v Univercity Jena bola Jena aj cast ORG, ale
  zaroven aj LOC)
- Xpart (cast je NER nazov) a Xderiv (odvodene slova) vyhodene
- v anotovani su nezrovnalosti, nemecky napr. anotuju omnoho viac javov ako anglicky (vratane napr. nazvov mien,
  znaciek)
- vsetky konvertovat na BIO + PLOM
- odstranene vety, kde viac ako 50% tvorili cisla

## DEP
- podobne ako pri POS beriem do uvahy tie slova z UD, ktore maju svoje id. Rozlicne zhluky neberieme do uvahy
- z labelov vztahov berieme len prvu cast, druhu language-specific ignorujeme

# NLI
- odstranili sme vsetky dvojice bez golden tagu

# LMO
- europarl, english zo spanielskej verzie
- train/dev/test 95 - 2.5 - 2.5


# Semantic Role Labeling
Velmi zaujimavo vyzerajuci task, ale dostupne data stoja 2000+ eur.


# Klasifikacia dokumentov
Tento task predbezne zavrhujem. Vidim tam len maly suvis s inymi ulohami. Empiricky sa navyse ukazuje,
ze bag of words pristupy su tu stale top, co nenaznacuje ze by LSTM model mohol byt aj s inymi datami
velmi uspesny.
Mozny zdroj dat: A Corpus for Multilingual Document Classification in Eight Languages


# NLI anglicke vysledky
Pre anglictinu davam 82%, co je na urovni jednoducheho porovnatelneho baseline (stranka SNLI)


# Learning rate scheduling
Experimentoval som s exponential decay:
0.95 exp decay mal horsie vysledky
0.99 a static mali prakticky rovnake


# Konvolucna siet na znakoch
Nedokazal som prekonat vysledky LSTM, pricom som zaznamenal len male zrychlenie (cca 2%).
Je tazke ale vystopovat, ake su best practices pri takomto modeli.


# Vysledky z grid search

Adam vs. RMSProp - Pri dependency parsingu bol jednoznacne lepsi Adam.
Pri POS bol lepsi vo vacsine pripadov, aj ked vysledky sa lisili len malo.
Pri NER vysledky skakali hore dole a tazko rozhodnut, co bolo lepsie.
Zaver: ADAM

Batch size 8 16 32
Pri DEP jednoznacne dominuje 32, prave tu si ale vacsi nemozno dovolit
Pri POS to sa to dost myli, ale zda sa ze vacsi batch_size dominuje pri mensom learning_rate
Pri NER podobne ako POS, aj ked dominancia pri mensom learning_rate je este viac viditelna
Dominancia POS a NER mozu byt sposobena mensim poctom epoch. Viac dat sa tak pri malom learning_rate moze dost osvedcit.
Zaver: 32, skusime experimentovat s vacsim poctom navzdory DEP
Update: Viac sa ukazalo pre POS aj NER ako nevhodne. 32 je nateraz finalne cislo.

Learning rate 0.003, 0.001, 0.0003
0.0003 sa ukazalo ako jednoznacne maly learning rate.
0.003 sa zda byt najlepsi, ALE treba skusit mono este vacsi.
A zaroven, vo viacerych pripadoch nebol model dotrenovany.
Zaver: Treba dalsie experimenty, predbezne nastavujem learning_rate na 0.003

Batch size aj learning rate sa ukazali ako pomerne citlive parametre. Na toto si treba dat pozor.
DP bol nedotrenovany, 30 epoch moze byt prilis malo.

# Float16
Chcel som skusit namiesto float32 pouzit float16. Zo znizenymi pamatovymi narokmi by som potom mozno dokazal spustat
DEP aj s vacsou batch size. Bohuzial viacere komponenty z contribu float16 vobec nepripustaju, napr. CUDNN LSTM alebo
CRF loss. Stalo by za to pozriet sa, ci toto nie je adresovane v novsej verzii tensorflow.

# Rozlicne casy spracovania datasetov
Pri NER-es a NER-de som spozoroval velky rozdiel v case spracovania epochy. Spanielske vety su ale na dlzku v priemere
2x take dlhe. Je to zrejme sposobene rozlicnou domenou datasetu.

# ML MT experiment
Vysledky z ML MT su prekvapivo zle. Vo vela ulohach sa nachadzaju na takej urovni, aku SL ST dosiahlo po par epochach.
Model ma zrejme velke problemy vysporiadat sa so zmenami v domene a skonvergovat ku niecomu inteligentnemu.
Niekolko napadov ako to zachranit:
1) Experiment kde jeden jazyk bude low resource
2) Experiment kde sa jeden jazyk bude vyberat viac ako ostatne
3) Experiment kde sa natrenuje ML MT model a potom sa dotrenuje pre jednotlive ulohy
4) Pre istotu iba ML a MT experimenty
5) learning_rate tuning

Ad 1: Pri cestine sa ukazalo ze niekde medzi 500 a 5000 training samplov sa viac oplati pouzit MLMT pristup.
Pozitivny transfer tam teda prebieha, ale len pri low resource. Inak je stale vyhodnejsie mat data z identickej domeny.
Pri fine-tuningu dokazem SL-ST porazat aj pri 5-15k training samploch.

Ad 3: Pre prilis malo dat to vysledky pochopitelne zhorsilo. Pre stredne mnozstvo to jasne pomohlo.
Pre velke mnozstvo dat to vela nespravilo. Moze byt v poslednom pripade problem s tym, ze sa s modelom inicializuju aj
Adam-related parametre ktore potom spomalia ucenie?

Ad 4: Pre DEP a NER sa lepsie ako MLMT javi cisto ML. Problem ale moze byt v tom, ze pri MT a MLMT obmedzujem velkost
in-language uloh.

Stale sa ukazuje, ze tie modely su nedotrenovane. Namiesto fixneho casu treba pokial je to mozne zaviest early stopping.

Pri dotrenovani nezalezi na tom, ci zresetujem Adam-related premenne. Vysledky su prakticky totozne.

Co treba skusit:
- MLMT bez taskov, ktore nijako nesuvisia s target taskom (chceme porazit ML)
- SLST s lr resetom
- MLMT a MT s tym ze obmedzuje iba velkost target task dat (chceme porazit ML)
- skus MLMT este dotrenovat
- treba sa pozriet preco je velky rozdiel medzi dev a test a ci by ich vymena zamavala poradim
- MLMT ma problem fitnut training data, mozno pomoze lepsi task sampling

LOSS 500
Pri POS sa sprava ako sa ocakava - po prechode na fine tuning zacne rapidne stupat, pricom skore klesa
Pri NER azcne stupat najme pri ML*, ale skore sa aj tak zlepsuje pri fine tuningu.
Pri DEP sa tiez vyrazne zlepsi skore, napriek stupajucej chybe.

# zavery
- ML sa ukazalo velmi silne, napriek tomu ze mame pomerne cudzie jazyky
- MT treba lepsi pristup, toto takyto jednoduchy mdoel nezvlada
- MLMT trpi nedostatkami MT, ale zaroven prebera z ML dobre vysledky - chcelo by to s tym MT zatocit
- pre POS a DEP mame sota vysledky, ale SLST modelom

# Predspracovanie
Pri prezreti slov, ktore sa nenasli v embeddingoch mi naslo vela slov, ktore
- boli spojene spojovnikom (toto najma pri LMO - tu by sa to dalo rozbit)
- boli cislami (v MUSE je tag # - su to cisla?)
- kusky textu s apostrofami ('s, 'll) - toto je v mUSE rozbite na <<'>> a <<ll>> napr.
- taktiez slova s bodkami <<mr.>>, <<u.s.>>
- bolo by fajn pozriet sa na to a zjednotit to, problem je, ze v POS tagoch napr. potom nie su tagy pre rozdelene
  slova a pod.

# Co mozno skumat v MLMT nastaveni?
- vyvoj loss / performance
- aktivacie na word a sentence-level LSTM layeroch - pouzivajus a rovnomerne? ako s tym zamava adversarial learning?
  vizualizacie
- matice, ktore nasleduju za tymito layermi - pouzivaju neurony rovnomerne? nerozdeli si model parameter space pre
  jednotlive ulohy? ako sa budu modely chovat pri zvysovani kapacity (MLMT by z toho mal profitovat najviac)
  - tato matica by sa mala dat do modelu pridat v configu, inak by tam nemusela byt (je zhorsenie casovej narocnosti
    pridania takejto matice badatelne?)
- ake su techniky na analyzu takychto modelov? zakladne statistiky, SVCCA, co este?
- mozeme sledovat, ako sa vyvija performance na nesuvisiacich taskoch. mozeme predpokladat ze pri ML sa to bude
  zlepsovat, aj ked model dany jazyk nikdy nevidel?
- ako reaguju rozlicne nastavenia na rozlicnu kapacitu word-level LSTM. mozeme predpokladat, ze pri viacerych taskoch
  bude vediet model vacsiu kapacitu lepsie vyuzit
- gradient norms

# LMO Cost
sparse softmax with cross entropy vyzera byt velmi dobre optimalizovany, NCE ani samples softmax s 10 samplovaniami
nie su pri trenovani z nejakeho dovodu rychlejsie.

# DEP Edmond's algorithm
mierne vylepsenie vysledkov, ale najma naozaj vraciame parse tree a nie nejaky pseudo-parse


# CURRENT EXPERIMENTS
11.12.2018 martak: vanilla - stsl, ml, mt, mtml
13.12.2018 gcp - ml w/o adversarial

# GCP notes
added drivers repo
sudo apt install nvidia-driver-415
reboot
check nvidia-smi
apt install python3-pip python3-dev
pip3 -V
install cuda per tf guide
install tf

check tf install:
from tensorflow.python.client import device_lib
device_lib.list_local_devices()

#gsutil je predinstalovany
gsutil cp -r gs://cll ~ (100MB/s speed)
bucket cez browser (centy za hodinu)

sudo nvidia-smi -pm 1

sudo poweroff
