# Preprocessing

Slova pri spracovani na urovni znakov berieme do maximalnej dlzky 30.
Dlzka vety je maximalne 70, minimalne 2.

## POS
- vsetko z UD datasetu
- pri CZ sme spojili vsetky train do jedneho
- niektore mali rozlicne spojene slova (de el > del v spanielcine), tu sme zobrali len rozobrane slova.
- v anglickom boli niektore slova presunute, pokial sa na to odkazovalo v text neskor (8.1), tieto som ignoroval

## NER
- english - CoNLL'03 (PER, LOC, ORG, MISC)
    - toto neplati, zobral som nejaky iny dataset a dal som ho 80-10-10 - treba pozriet preprocessing script
- spanish - CoNLL'02 (PER, LOC, ORG, MISC)
- german - germeval (PER, LOC, ORG, MISC)
- czech - czech named entity corpus (p - PER, g - LOC, i+ms - ORG, oa+op - MISC)
- only first level annotation (v Germeval mali aj second level, cize napr. v Univercity Jena bola Jena aj cast ORG, ale zaroven aj LOC)
- Xpart (cast je NER nazov) a Xderiv (odvodene slova) vyhodene
- v anotovani su nezrovnalosti, nemecky napr. anotuju omnoho viac javov ako anglicky (vratane napr. nazvov mien, znaciek)
- vsetky konvertovat na BIO + PLOM
- odstranene vety, kde viac ako 50% tvorili cisla

## DEP
- podobne ako pri POS beriem do uvahy tie slova z UD, ktore maju svoje id. Rozlicne zhluky neberieme do uvahy
- z labelov vztahov berieme len prvu cast, druhu language-specific ignorujeme

# NLI
- odstranili sme vsetky dvojice bez golden tagu

# LMO
- europarl, english zo spanielskej verzie
- train/dev/test 95 - 2.5 - 2.5


# Semantic Role Labeling
Velmi zaujimavo vyzerajuci task, ale dostupne data stoja 2000+ eur.


# Klasifikacia dokumentov
Tento task predbezne zavrhujem. Vidim tam len maly suvis s inymi ulohami. Empiricky sa navyse ukazuje,
ze bag of words pristupy su tu stale top, co nenaznacuje ze by LSTM model mohol byt aj s inymi datami
velmi uspesny.
Mozny zdroj dat: A Corpus for Multilingual Document Classification in Eight Languages


# NLI anglicke vysledky
Pre anglictinu davam 82%, co je na urovni jednoducheho porovnatelneho baseline (stranka SNLI)


# Learning rate scheduling
Experimentoval som s exponential decay:
0.95 exp decay mal horsie vysledky
0.99 a static mali prakticky rovnake


# Konvolucna siet na znakoch
Nedokazal som prekonat vysledky LSTM, pricom som zaznamenal len male zrychlenie (cca 2%).
Je tazke ale vystopovat, ake su best practices pri takomto modeli.


# Vysledky z grid search

Adam vs. RMSProp - Pri dependency parsingu bol jednoznacne lepsi Adam.
Pri POS bol lepsi vo vacsine pripadov, aj ked vysledky sa lisili len malo.
Pri NER vysledky skakali hore dole a tazko rozhodnut, co bolo lepsie.
Zaver: ADAM

Batch size 8 16 32
Pri DEP jednoznacne dominuje 32, prave tu si ale vacsi nemozno dovolit
Pri POS to sa to dost myli, ale zda sa ze vacsi batch_size dominuje pri mensom learning_rate
Pri NER podobne ako POS, aj ked dominancia pri mensom learning_rate je este viac viditelna
Dominancia POS a NER mozu byt sposobena mensim poctom epoch. Viac dat sa tak pri malom learning_rate moze dost osvedcit.
Zaver: 32, skusime experimentovat s vacsim poctom navzdory DEP
Update: Viac sa ukazalo pre POS aj NER ako nevhodne. 32 je nateraz finalne cislo.

Learning rate 0.003, 0.001, 0.0003
0.0003 sa ukazalo ako jednoznacne maly learning rate.
0.003 sa zda byt najlepsi, ALE treba skusit mono este vacsi.
A zaroven, vo viacerych pripadoch nebol model dotrenovany.
Zaver: Treba dalsie experimenty, predbezne nastavujem learning_rate na 0.003

Batch size aj learning rate sa ukazali ako pomerne citlive parametre. Na toto si treba dat pozor.
DP bol nedotrenovany, 30 epoch moze byt prilis malo.

# Float16
Chcel som skusit namiesto float32 pouzit float16. Zo znizenymi pamatovymi narokmi by som potom mozno dokazal spustat
DEP aj s vacsou batch size. Bohuzial viacere komponenty z contribu float16 vobec nepripustaju, napr. CUDNN LSTM alebo
CRF loss. Stalo by za to pozriet sa, ci toto nie je adresovane v novsej verzii tensorflow.

# Rozlicne casy spracovania datasetov
Pri NER-es a NER-de som spozoroval velky rozdiel v case spracovania epochy. Spanielske vety su ale na dlzku v priemere
2x take dlhe. Je to zrejme sposobene rozlicnou domenou datasetu.

# ML MT experiment
Vysledky z ML MT su prekvapivo zle. Vo vela ulohach sa nachadzaju na takej urovni, aku SL ST dosiahlo po par epochach.
Model ma zrejme velke problemy vysporiadat sa so zmenami v domene a skonvergovat ku niecomu inteligentnemu.
Niekolko napadov ako to zachranit:
1) Experiment kde jeden jazyk bude low resource
2) Experiment kde sa jeden jazyk bude vyberat viac ako ostatne
3) Experiment kde sa natrenuje ML MT model a potom sa dotrenuje pre jednotlive ulohy
4) Pre istotu iba ML a MT experimenty
5) learning_rate tuning


Treba si dat pozor, lebo max_dt_size funguje iba na ceske datasety