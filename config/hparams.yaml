default: &default

  word_emb_type: mwe # mwe / random / mwe_projected
  word_emb_size: 300
  train_emb: false
  char_emb_size: 30
  char_level: true

  word_lstm_size: 200
  char_lstm_size: 100
  hidden_size: 200 # universal hidden size used for task-specific computations

  learning_rate: 0.003
  learning_rate_decay: 0.99
  learning_rate_schedule: decay  # static / decay
  batch_size: 32
  epoch_steps: 2048
  epochs: 30
  clip: 1
  dropout: 0.5
  optimizer: adam  # rmsprop / adam / adagrad / sgd

  limited_data_size: -1  # max number of samples read from a file (-1 is unlimited)
  limited_language: na
  limited_task_language: na
  limited_task: na
  eval_size_limit: -1  # for debugging

  train_only: na
  focus_on: na
  focus_rate: 0.5

  max_sentence_length: 70
  min_sentence_length: 2
  max_word_length: 20
  min_word_freq: 1
  min_char_freq: 100
  max_dataset_cache: 200000

  word_lstm_private: false
  word_lstm_task: false
  word_lstm_lang: false
  word_lstm_global: true

  task_layer_private: false

  adversarial_training: false
  adversarial_lambda: 0.5
  adversarial_freq: 1

  frobenius: 0.0

  lmo_vocab_limit: 15000

  # experiment specific settings

  # settings
  tasks: null
  setup: default
  use_gpu: true
  show_graph: false
  save_model: never  # never / epoch / run
  load_model: na
  allow_gpu_growth: false

debug:
  <<: *default
  word_lstm_size: 10
  char_lstm_size: 10
  batch_size: 3
  epoch_steps: 10
  epochs: 5
  eval_size_limit: 100

production:
  <<: *default
  save_model: epoch