default: &default

  word_emb_type: static
  word_emb_size: 300
  train_emb: false
  char_emb_size: 30
  char_level: true

  task_layer_sharing: true
  word_lstm_size: 200
  char_lstm_size: 100
  hidden_size: 200 # universal hidden size used for task-specific computations

  learning_rate: 0.003
  learning_rate_decay: 0.99
  learning_rate_schedule: decay # static/decay
  batch_size: 32
  epoch_steps: 1024
  epochs: 30
  clip: 1
  dropout: 0.5
  optimizer: adam # rmsprop/adam/adagrad/sgd

  limited_data_size: -1 # max number of samples read from a file (-1 is unlimited)
  limited_language: na
  limited_task_language: na
  limited_task: na
  train_only: na

  max_sentence_length: 70
  min_sentence_length: 2
  max_word_length: 20
  min_word_freq: 1
  min_char_freq: 100
  max_dataset_cache: 200000

  lmo_vocab_limit: 15000

  # experiment specific settings

  # settings
  tasks: null
  setup: default
  use_gpu: true
  show_graph: false
  save_parameters: false
  saved_model: na

debug:
  <<: *default
  word_lstm_size: 10
  char_lstm_size: 10
  batch_size: 3
  epoch_steps: 10
  epochs: 5

production:
  <<: *default