default: &default

  word_emb_type: static
  word_emb_size: 300
  train_emb: false
  char_emb_size: 30
  char_level: true

  crf_sharing: true
  word_lstm_size: 100
  char_lstm_size: 100

  learning_rate: 0.003
  learning_rate_decay: 0.99
  learning_rate_schedule: decay # static/decay
  batch_size: 32
  epoch_steps: 1024
  epochs: 30
  clip: 1
  dropout: 0.5
  optimizer: adam # rmsprop/adam/adagrad/sgd

  setup: default
  tasks: null

  max_sentence_length: 70
  min_sentence_length: 2

  lmo_vocab_size: 10000 # how many words are output vocabulary for language modeling
  max_dt_size: 0 # max number of samples read from a file (0 is unlimited)

  # settings
  use_gpu: true
  show_graph: false
  save_parameters: false
  saved_model: na

  char_level_type: rnn # / cnn - po experimente vyhodit

debug:
  <<: *default
  word_lstm_size: 10
  char_lstm_size: 10
  batch_size: 8
  epoch_steps: 10
  epochs: 5

production:
  <<: *default